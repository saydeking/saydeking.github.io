@inproceedings{king_exploring_2025,
	title = {Exploring {Vision}-{Based} {Features} for {Detecting} {Deception} in {Well}-{Being}: {A} {Cross}-{Domain} {Comparison}},
	copyright = {All rights reserved},
	shorttitle = {Exploring {Vision}-{Based} {Features} for {Detecting} {Deception} in {Well}-{Being}},
	url = {https://ieeexplore.ieee.org/abstract/document/11099290},
	doi = {10.1109/FG61629.2025.11099290},
	abstract = {Deception detection has been extensively studied using vision-based features in domains such as crime, finance, and social interaction. However, little attention has been given to how deception manifests visually in self-reported well-being-a critical area for behavioral health, where inaccurate reporting may affect treatment outcomes and the therapeutic alliance. While clinicians often rely on visual cues such as gaze, facial expressions, and body language to assess deception, these cues remain underutilized in AI-based deception detection in wellbeing scenarios. This study explores vision-based features of deception in the well-being domain and compares them with those from three other domains: biography, academics, and crime. Using mock interview data, we extract facial landmarks, body gestures, and facial action units (AUs) using four feature selection methods. We then visualize and analyze the spatial distribution of features associated with truthful and deceptive responses. Results show that well-being features are generally fewer and more localized-particularly around the nose ridge-with unique presence of eye landmarks and limited hand gestures. In contrast, biography and academics show broader facial and body engagement, while crime displays no differentiation between truth- and deception-related features and lacks emotional AU combinations. AUs associated with joy (AU 6 and AU 12) appear consistently across well-being, academics, and biography, suggesting some domain-agnostic cues. Overall, our findings indicate that most visual features relevant to deception are domain-specific. This highlights the importance of contextaware approaches in deception modeling and supports the development of more reliable, human-centered AI tools for wellbeing assessment and mental health applications.},
	urldate = {2025-10-25},
	booktitle = {2025 {IEEE} 19th {International} {Conference} on {Automatic} {Face} and {Gesture} {Recognition} ({FG})},
	author = {King, Sayde L. and Neal, Tempestt},
	month = may,
	year = {2025},
	note = {ISSN: 2770-8330},
	keywords = {Biographies, Feature extraction, Gesture recognition, Graphical models, Hands, Interviews, Mental health, Nose, Reliability, Visualization},
	pages = {1--10},
	file = {Snapshot:/Users/saydeking/Zotero/storage/PWI348KC/11099290.html:text/html},
}

@inproceedings{king_evaluating_2026,
	address = {Cham},
	title = {Evaluating {Visual} and {Behavioral} {Signals} of {Deception} in {Real}-{World} {Contexts}},
	copyright = {All rights reserved},
	isbn = {978-3-032-07715-8},
	doi = {10.1007/978-3-032-07715-8_13},
	abstract = {Deception detection remains a critical challenge for security, human-centered computing, and behavioral research, yet most existing systems struggle to generalize across real-world contexts. This paper examines multimodal deception detection across four domains–academics, biography, well-being, and crime. Using 400 videos from two datasets, including a new one we have released, we evaluate facial, affective, gaze, and gesture features in single- and cross-domain settings. Gaze and gesture consistently outperform facial and affective features, especially in high-risk or narrative contexts, despite being selected less often. Facial features, though common, often lack predictive value, underscoring the need to prioritize performance over frequency. We find that expressivity, not response length, improves detectability, and generalization is stronger when domains share risk or narrative structure. These insights support the design of adaptable, context-aware deception detection systems.},
	language = {en},
	booktitle = {Social, {Cultural}, and {Behavioral} {Modeling}},
	publisher = {Springer Nature Switzerland},
	author = {King, Sayde L. and Abootalebi, Hoorad and Wai, Georgia Ng and Neal, Tempestt},
	editor = {Thomson, Robert and Renshaw, Scott and Al-khateeb, Samer and Burger, Annetta and Park, Patrick and Pyke, Aryn A.},
	year = {2026},
	keywords = {cross-domain, deception detection, feature analysis, machine learning, video},
	pages = {131--141},
}

@proceeding{king2020learning,
  title={Learning a Privacy-Preserving Global Feature Set for Mood Classification Using Smartphone Activity and Sensor Data},
  author={King, Sayde and Ebraheem, Mohamed and Zanna, Khadija and Neal, Tempestt},
  booktitle={2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)},
  pages={582--586},
  year={2020},
  organization={IEEE}
}


@proceeding{ebraheem2022lip,
  title={Lip Movement as a WiFi-Enabled Behavioral Biometric: A Pilot Study},
  author={Ebraheem, Mohamed and King, Sayde and Neal, Tempestt},
  booktitle={International Conference on Human-Computer Interaction},
  pages={473--480},
  year={2022},
  organization={Springer International Publishing Cham}
}


@proceeding{loecher2023assessing,
  title={Assessing the Efficacy of a Self-Stigma Reduction Mental Health Program with Mobile Biometrics: Work-in-Progress},
  author={Loecher, Nele and King, Sayde and Cabo, Joseph and Neal, Tempestt and Kosyluk, Kristin},
  booktitle={2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG)},
  pages={1--6},
  year={2023},
  organization={IEEE}
}


@proceeding{lozano2023observations,
  title={Observations of Caregivers of Persons with Dementia: A Qualitative Study to Assess the Feasibility of Behavior Recognition Using AI for Supporting At-Home Care},
  author={Lozano, Wilson and King, Sayde and Neal, Tempestt},
  booktitle={International Conference on Human-Computer Interaction},
  pages={331--344},
  year={2023},
  organization={Springer Nature Switzerland Cham}
}

@proceeding{king2023therapist,
  title={Therapist Perceptions of Automated Deception Detection in Mental Health Applications},
  author={King, Sayde L and Johnson, Najare and Kosyluk, Kristin and Neal, Tempestt},
  booktitle={International Conference on Human-Computer Interaction},
  pages={83--97},
  year={2023},
  organization={Springer Nature Switzerland Cham}
}

@INPROCEEDINGS{10581939,
  author={King, Sayde and Ebraheem, Mohamed and Dang, Phuong and Neal, Tempestt},
  booktitle={2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG)}, 
  title={Toward Emotion Recognition and Person Identification Using Lip Movement from Wireless Signals: A Preliminary Study}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  keywords={Wireless communication;Emotion recognition;Lips;Frequency-domain analysis;Mouth;Speech recognition;Feature extraction},
  doi={10.1109/FG59268.2024.10581939},
  organization={IEEE}}


@proceeding{zanna2019studying,
  title={Studying the impact of mood on identifying smartphone users},
  author={Zanna, Khadija and King, Sayde and Neal, Tempestt and Canavan, Shaun},
  booktitle={arXiv preprint arXiv:1906.11960},
  year={2019}
}
